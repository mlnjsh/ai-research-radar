[
  {
    "arxiv_id": "2602.20144v1",
    "title": "Agentic AI for Scalable and Robust Optical Systems Control",
    "authors": [
      "Zehao Wang",
      "Mingzhe Han",
      "Wei Cheng",
      "Yue-Kai Huang",
      "Philip Ji",
      "Denton Wu",
      "Mahdi Safari",
      "Flemming Holtorf",
      "Kenaish AlQubaisi",
      "Norbert M. Linke",
      "Danyang Zhuo",
      "Yiran Chen",
      "Ting Wang",
      "Dirk Englund",
      "Tingjun Chen"
    ],
    "abstract": "We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordination, robustness to linguistic variation, and error handling. We assess two deployment configurations--commercial online LLMs and locally hosted open-source LLMs--and compare them with LLM-based code generation baselines. AgentOptics achieves 87.7%--99.0% average task success rates, significantly outperforming code-generation approaches, which reach up to 50% success. We further demonstrate broader applicability through five case studies extending beyond device-level control to system orchestration, monitoring, and closed-loop optimization. These include DWDM link provisioning and coordinated monitoring of coherent 400 GbE and analog radio-over-fiber (ARoF) channels; autonomous characterization and bias optimization of a wideband ARoF link carrying 5G fronthaul traffic; multi-span channel provisioning with launch power optimization; closed-loop fiber polarization stabilization; and distributed acoustic sensing (DAS)-based fiber monitoring with LLM-assisted event detection. These results establish AgentOptics as a scalable, robust paradigm for autonomous control and orchestration of heterogeneous optical systems.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "eess.SY",
    "published": "2026-02-23T18:54:32Z",
    "updated": "2026-02-23T18:54:32Z",
    "url": "https://arxiv.org/abs/2602.20144v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20144v1",
    "relevance_score": 20.0,
    "display_category": "LLMs & Agents",
    "summary": "We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP).",
    "keyword_matches": [
      "RAG",
      "agentic"
    ]
  },
  {
    "arxiv_id": "2602.20161v1",
    "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device",
    "authors": [
      "Abdelrahman Shaker",
      "Ahmed Heakl",
      "Jaseel Muhammad",
      "Ritesh Thawkar",
      "Omkar Thawakar",
      "Senmao Li",
      "Hisham Cholakkal",
      "Ian Reid",
      "Eric P. Xing",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ],
    "abstract": "Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "published": "2026-02-23T18:59:58Z",
    "updated": "2026-02-23T18:59:58Z",
    "url": "https://arxiv.org/abs/2602.20161v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20161v1",
    "relevance_score": 10.0,
    "display_category": "Computer Vision",
    "summary": "Unified multimodal models can both understand and generate visual content within a single architecture.",
    "keyword_matches": [
      "RAG"
    ]
  },
  {
    "arxiv_id": "2602.20160v1",
    "title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction",
    "authors": [
      "Chen Wang",
      "Hao Tan",
      "Wang Yifan",
      "Zhiqin Chen",
      "Yuheng Liu",
      "Kalyan Sunkavalli",
      "Sai Bi",
      "Lingjie Liu",
      "Yiwei Hu"
    ],
    "abstract": "We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model's capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "published": "2026-02-23T18:59:45Z",
    "updated": "2026-02-23T18:59:45Z",
    "url": "https://arxiv.org/abs/2602.20160v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20160v1",
    "relevance_score": 10.0,
    "display_category": "Computer Vision",
    "summary": "We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational...",
    "keyword_matches": [
      "RAG"
    ]
  },
  {
    "arxiv_id": "2602.20159v1",
    "title": "A Very Big Video Reasoning Suite",
    "authors": [
      "Maijunxian Wang",
      "Ruisi Wang",
      "Juyi Lin",
      "Ran Ji",
      "Thaddäus Wiedemer",
      "Qingying Gao",
      "Dezhi Luo",
      "Yaoyao Qian",
      "Lianyu Huang",
      "Zelong Hong",
      "Jiahui Ge",
      "Qianli Ma",
      "Hang He",
      "Yifan Zhou",
      "Lingzi Guo",
      "Lantao Mei",
      "Jiachen Li",
      "Hanwen Xing",
      "Tianqi Zhao",
      "Fengyuan Yu",
      "Weihang Xiao",
      "Yizheng Jiao",
      "Jianheng Hou",
      "Danyang Zhang",
      "Pengcheng Xu",
      "Boyang Zhong",
      "Zehong Zhao",
      "Gaoyun Fang",
      "John Kitaoka",
      "Yile Xu",
      "Hua Xu",
      "Kenton Blacutt",
      "Tin Nguyen",
      "Siyuan Song",
      "Haoran Sun",
      "Shaoyue Wen",
      "Linyang He",
      "Runming Wang",
      "Yanzhi Wang",
      "Mengyue Yang",
      "Ziqiao Ma",
      "Raphaël Millière",
      "Freda Shi",
      "Nuno Vasconcelos",
      "Daniel Khashabi",
      "Alan Yuille",
      "Yilun Du",
      "Ziming Liu",
      "Bo Li",
      "Dahua Lin",
      "Ziwei Liu",
      "Vikash Kumar",
      "Yijiang Li",
      "Lei Yang",
      "Zhongang Cai",
      "Hokin Deng"
    ],
    "abstract": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "published": "2026-02-23T18:59:41Z",
    "updated": "2026-02-23T18:59:41Z",
    "url": "https://arxiv.org/abs/2602.20159v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20159v1",
    "relevance_score": 10.0,
    "display_category": "Computer Vision",
    "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored.",
    "keyword_matches": [
      "RAG"
    ]
  },
  {
    "arxiv_id": "2602.20151v1",
    "title": "Conformal Risk Control for Non-Monotonic Losses",
    "authors": [
      "Anastasios N. Angelopoulos"
    ],
    "abstract": "Conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage. The original algorithm controls the expected value of a loss that is monotonic in a one-dimensional parameter. Here, we present risk control guarantees for generic algorithms applied to possibly non-monotonic losses with multidimensional parameters. The guarantees depend on the stability of the algorithm -- unstable algorithms have looser guarantees. We give applications of this technique to selective image classification, FDR and IOU control of tumor segmentations, and multigroup debiasing of recidivism predictions across overlapping race and sex groups using empirical risk minimization.",
    "categories": [
      "stat.ME",
      "cs.LG",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "published": "2026-02-23T18:58:54Z",
    "updated": "2026-02-23T18:58:54Z",
    "url": "https://arxiv.org/abs/2602.20151v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20151v1",
    "relevance_score": 10.0,
    "display_category": "Machine Learning & Deep Learning",
    "summary": "Conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage.",
    "keyword_matches": [
      "RAG"
    ]
  },
  {
    "arxiv_id": "2602.20150v1",
    "title": "Simulation-Ready Cluttered Scene Estimation via Physics-aware Joint Shape and Pose Optimization",
    "authors": [
      "Wei-Cheng Huang",
      "Jiaheng Han",
      "Xiaohan Ye",
      "Zherong Pan",
      "Kris Hauser"
    ],
    "abstract": "Estimating simulation-ready scenes from real-world observations is crucial for downstream planning and policy learning tasks. Regretfully, existing methods struggle in cluttered environments, often exhibiting prohibitive computational cost, poor robustness, and restricted generality when scaling to multiple interacting objects. We propose a unified optimization-based formulation for real-to-sim scene estimation that jointly recovers the shapes and poses of multiple rigid objects under physical constraints. Our method is built on two key technical innovations. First, we leverage the recently introduced shape-differentiable contact model, whose global differentiability permits joint optimization over object geometry and pose while modeling inter-object contacts. Second, we exploit the structured sparsity of the augmented Lagrangian Hessian to derive an efficient linear system solver whose computational cost scales favorably with scene complexity. Building on this formulation, we develop an end-to-end real-to-sim scene estimation pipeline that integrates learning-based object initialization, physics-constrained joint shape-pose optimization, and differentiable texture refinement. Experiments on cluttered scenes with up to 5 objects and 22 convex hulls demonstrate that our approach robustly reconstructs physically valid, simulation-ready object shapes and poses.",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "published": "2026-02-23T18:58:24Z",
    "updated": "2026-02-23T18:58:24Z",
    "url": "https://arxiv.org/abs/2602.20150v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20150v1",
    "relevance_score": 10.0,
    "display_category": "Optimization & Engineering",
    "summary": "Estimating simulation-ready scenes from real-world observations is crucial for downstream planning and policy learning tasks.",
    "keyword_matches": [
      "RAG"
    ]
  },
  {
    "arxiv_id": "2602.20141v1",
    "title": "Recurrent Structural Policy Gradient for Partially Observable Mean Field Games",
    "authors": [
      "Clarisse Wibault",
      "Johannes Forkel",
      "Sebastian Towers",
      "Tiphaine Wibault",
      "Juan Duque",
      "George Whittle",
      "Andreas Schaab",
      "Yucheng Yang",
      "Chiyuan Wang",
      "Michael Osborne",
      "Benjamin Moll",
      "Jakob Foerster"
    ],
    "abstract": "Mean Field Games (MFGs) provide a principled framework for modeling interactions in large population models: at scale, population dynamics become deterministic, with uncertainty entering only through aggregate shocks, or common noise. However, algorithmic progress has been limited since model-free methods are too high variance and exact methods scale poorly. Recent Hybrid Structural Methods (HSMs) use Monte Carlo rollouts for the common noise in combination with exact estimation of the expected return, conditioned on those samples. However, HSMs have not been scaled to Partially Observable settings. We propose Recurrent Structural Policy Gradient (RSPG), the first history-aware HSM for settings involving public information. We also introduce MFAX, our JAX-based framework for MFGs. By leveraging known transition dynamics, RSPG achieves state-of-the-art performance as well as an order-of-magnitude faster convergence and solves, for the first time, a macroeconomics MFG with heterogeneous agents, common noise and history-aware policies. MFAX is publicly available at: https://github.com/CWibault/mfax.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "published": "2026-02-23T18:53:09Z",
    "updated": "2026-02-23T18:53:09Z",
    "url": "https://arxiv.org/abs/2602.20141v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20141v1",
    "relevance_score": 10.0,
    "display_category": "LLMs & Agents",
    "summary": "Mean Field Games (MFGs) provide a principled framework for modeling interactions in large population models: at scale, population dynamics become deterministic, with uncertainty entering only...",
    "keyword_matches": [
      "RAG"
    ]
  },
  {
    "arxiv_id": "2602.20135v1",
    "title": "KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration",
    "authors": [
      "Mohammad Amanlou",
      "Erfan Shafiee Moghaddam",
      "Yasaman Amou Jafari",
      "Mahdi Noori",
      "Farhan Farsi",
      "Behnam Bahrak"
    ],
    "abstract": "With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "published": "2026-02-23T18:46:27Z",
    "updated": "2026-02-23T18:46:27Z",
    "url": "https://arxiv.org/abs/2602.20135v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20135v1",
    "relevance_score": 10.0,
    "display_category": "LLMs & Agents",
    "summary": "With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG).",
    "keyword_matches": [
      "RAG"
    ]
  },
  {
    "arxiv_id": "2602.20117v1",
    "title": "ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models",
    "authors": [
      "Andre He",
      "Nathaniel Weir",
      "Kaj Bostrom",
      "Allen Nie",
      "Darion Cassel",
      "Sam Bayless",
      "Huzefa Rangwala"
    ],
    "abstract": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "published": "2026-02-23T18:34:29Z",
    "updated": "2026-02-23T18:34:29Z",
    "url": "https://arxiv.org/abs/2602.20117v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20117v1",
    "relevance_score": 10.0,
    "display_category": "Machine Learning & Deep Learning",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers.",
    "keyword_matches": [
      "RAG"
    ]
  },
  {
    "arxiv_id": "2602.20114v1",
    "title": "Benchmarking Unlearning for Vision Transformers",
    "authors": [
      "Kairan Zhao",
      "Iurie Luca",
      "Peter Triantafillou"
    ],
    "abstract": "Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for VTs. This work is the first to attempt this, benchmarking MU algorithm performance in different VT families (ViT and Swin-T) and at different capacities. The work employs (i) different datasets, selected to assess the impacts of dataset scale and complexity; (ii) different MU algorithms, selected to represent fundamentally different approaches for MU; and (iii) both single-shot and continual unlearning protocols. Additionally, it focuses on benchmarking MU algorithms that leverage training data memorization, since leveraging memorization has been recently discovered to significantly improve the performance of previously SOTA algorithms. En route, the work characterizes how VTs memorize training data relative to CNNs, and assesses the impact of different memorization proxies on performance. The benchmark uses unified evaluation metrics that capture two complementary notions of forget quality along with accuracy on unseen (test) data and on retained data. Overall, this work offers a benchmarking basis, enabling reproducible, fair, and comprehensive comparisons of existing (and future) MU algorithms on VTs. And, for the first time, it sheds light on how well existing algorithms work in VT settings, establishing a promising reference performance baseline.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "published": "2026-02-23T18:33:16Z",
    "updated": "2026-02-23T18:33:16Z",
    "url": "https://arxiv.org/abs/2602.20114v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20114v1",
    "relevance_score": 10.0,
    "display_category": "Computer Vision",
    "summary": "Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI.",
    "keyword_matches": [
      "RAG"
    ]
  },
  {
    "arxiv_id": "2602.20094v1",
    "title": "CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching",
    "authors": [
      "Yuzhe Wang",
      "Yaochen Zhu",
      "Jundong Li"
    ],
    "abstract": "As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models' reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "published": "2026-02-23T18:06:15Z",
    "updated": "2026-02-23T18:06:15Z",
    "url": "https://arxiv.org/abs/2602.20094v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20094v1",
    "relevance_score": 10.0,
    "display_category": "LLMs & Agents",
    "summary": "As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious...",
    "keyword_matches": [
      "RAG"
    ]
  },
  {
    "arxiv_id": "2602.20091v1",
    "title": "How Retrieved Context Shapes Internal Representations in RAG",
    "authors": [
      "Samuel Yeh",
      "Sharon Li"
    ],
    "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediate information integration in RAG. In this work, we study RAG through the lens of latent representations. We systematically analyze how different types of retrieved documents affect the hidden states of LLMs, and how these internal representation shifts relate to downstream generation behavior. Across four question-answering datasets and three LLMs, we analyze internal representations under controlled single- and multi-document settings. Our results reveal how context relevancy and layer-wise processing influence internal representations, providing explanations on LLMs output behaviors and insights for RAG system design.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2026-02-23T18:02:04Z",
    "updated": "2026-02-23T18:02:04Z",
    "url": "https://arxiv.org/abs/2602.20091v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20091v1",
    "relevance_score": 10.0,
    "display_category": "LLMs & Agents",
    "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial.",
    "keyword_matches": [
      "RAG"
    ]
  },
  {
    "arxiv_id": "2602.20084v1",
    "title": "Do Large Language Models Understand Data Visualization Principles?",
    "authors": [
      "Martin Sinnona",
      "Valentin Bonas",
      "Viviana Siless",
      "Emmanuel Iarussi"
    ],
    "abstract": "Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (VLMs) can reason about and enforce visualization principles directly. Constraint based systems encode these principles as logical rules for precise automated checks, but translating them into formal specifications demands expert knowledge. This motivates leveraging LLMs and VLMs as principle checkers that can reason about visual design directly, bypassing the need for symbolic rule specification. In this paper, we present the first systematic evaluation of both LLMs and VLMs on their ability to reason about visualization principles, using hard verification ground truth derived from Answer Set Programming (ASP). We compiled a set of visualization principles expressed as natural-language statements and generated a controlled dataset of approximately 2,000 Vega-Lite specifications annotated with explicit principle violations, complemented by over 300 real-world Vega-Lite charts. We evaluated both checking and fixing tasks, assessing how well models detect principle violations and correct flawed chart specifications. Our work highlights both the promise of large (vision-)language models as flexible validators and editors of visualization designs and the persistent gap with symbolic solvers on more nuanced aspects of visual perception. They also reveal an interesting asymmetry: frontier models tend to be more effective at correcting violations than at detecting them reliably.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "published": "2026-02-23T17:51:06Z",
    "updated": "2026-02-23T17:51:06Z",
    "url": "https://arxiv.org/abs/2602.20084v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20084v1",
    "relevance_score": 10.0,
    "display_category": "Computer Vision",
    "summary": "Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication.",
    "keyword_matches": [
      "RAG"
    ]
  },
  {
    "arxiv_id": "2602.20078v1",
    "title": "Descent-Guided Policy Gradient for Scalable Cooperative Multi-Agent Learning",
    "authors": [
      "Shan Yang",
      "Yang Liu"
    ],
    "abstract": "Scaling cooperative multi-agent reinforcement learning (MARL) is fundamentally limited by cross-agent noise: when agents share a common reward, the actions of all $N$ agents jointly determine each agent's learning signal, so cross-agent noise grows with $N$. In the policy gradient setting, per-agent gradient estimate variance scales as $Θ(N)$, yielding sample complexity $\\mathcal{O}(N/ε)$. We observe that many domains -- cloud computing, transportation, power systems -- have differentiable analytical models that prescribe efficient system states. In this work, we propose Descent-Guided Policy Gradient (DG-PG), a framework that constructs noise-free per-agent guidance gradients from these analytical models, decoupling each agent's gradient from the actions of all others. We prove that DG-PG reduces gradient variance from $Θ(N)$ to $\\mathcal{O}(1)$, preserves the equilibria of the cooperative game, and achieves agent-independent sample complexity $\\mathcal{O}(1/ε)$. On a heterogeneous cloud scheduling task with up to 200 agents, DG-PG converges within 10 episodes at every tested scale -- from $N=5$ to $N=200$ -- directly confirming the predicted scale-invariant complexity, while MAPPO and IPPO fail to converge under identical architectures.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "published": "2026-02-23T17:45:08Z",
    "updated": "2026-02-23T17:45:08Z",
    "url": "https://arxiv.org/abs/2602.20078v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20078v1",
    "relevance_score": 10.0,
    "display_category": "LLMs & Agents",
    "summary": "Scaling cooperative multi-agent reinforcement learning (MARL) is fundamentally limited by cross-agent noise: when agents share a common reward, the actions of all $N$ agents jointly determine each...",
    "keyword_matches": [
      "multi-agent"
    ]
  },
  {
    "arxiv_id": "2602.20064v1",
    "title": "The LLMbda Calculus: AI Agents, Conversations, and Information Flow",
    "authors": [
      "Zac Garby",
      "Andrew D. Gordon",
      "David Sands"
    ],
    "abstract": "A conversation with a large language model (LLM) is a sequence of prompts and responses, with each response generated from the preceding conversation. AI agents build such conversations automatically: given an initial human prompt, a planner loop interleaves LLM calls with tool invocations and code execution. This tight coupling creates a new and poorly understood attack surface. A malicious prompt injected into a conversation can compromise later reasoning, trigger dangerous tool calls, or distort final outputs. Despite the centrality of such systems, we currently lack a principled semantic foundation for reasoning about their behaviour and safety. We address this gap by introducing an untyped call-by-value lambda calculus enriched with dynamic information-flow control and a small number of primitives for constructing prompt-response conversations. Our language includes a primitive that invokes an LLM: it serializes a value, sends it to the model as a prompt, and parses the response as a new term. This calculus faithfully represents planner loops and their vulnerabilities, including the mechanisms by which prompt injection alters subsequent computation. The semantics explicitly captures conversations, and so supports reasoning about defenses such as quarantined sub-conversations, isolation of generated code, and information-flow restrictions on what may influence an LLM call. A termination-insensitive noninterference theorem establishes integrity and confidentiality guarantees, demonstrating that a formal calculus can provide rigorous foundations for safe agentic programming.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.PL",
    "published": "2026-02-23T17:22:35Z",
    "updated": "2026-02-23T17:22:35Z",
    "url": "https://arxiv.org/abs/2602.20064v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20064v1",
    "relevance_score": 10.0,
    "display_category": "LLMs & Agents",
    "summary": "A conversation with a large language model (LLM) is a sequence of prompts and responses, with each response generated from the preceding conversation.",
    "keyword_matches": [
      "agentic"
    ]
  },
  {
    "arxiv_id": "2602.20059v1",
    "title": "Interaction Theater: A case of LLM Agents Interacting at Scale",
    "authors": [
      "Sarath Shekkizhar",
      "Adam Earle"
    ],
    "abstract": "As multi-agent architectures and agent-to-agent protocols proliferate, a fundamental question arises: what actually happens when autonomous LLM agents interact at scale? We study this question empirically using data from Moltbook, an AI-agent-only social platform, with 800K posts, 3.5M comments, and 78K agent profiles. We combine lexical metrics (Jaccard specificity), embedding-based semantic similarity, and LLM-as-judge validation to characterize agent interaction quality. Our findings reveal agents produce diverse, well-formed text that creates the surface appearance of active discussion, but the substance is largely absent. Specifically, while most agents ($67.5\\%$) vary their output across contexts, $65\\%$ of comments share no distinguishing content vocabulary with the post they appear under, and information gain from additional comments decays rapidly. LLM judge based metrics classify the dominant comment types as spam ($28\\%$) and off-topic content ($22\\%$). Embedding-based semantic analysis confirms that lexically generic comments are also semantically generic. Agents rarely engage in threaded conversation ($5\\%$ of comments), defaulting instead to independent top-level responses. We discuss implications for multi-agent interaction design, arguing that coordination mechanisms must be explicitly designed; without them, even large populations of capable agents produce parallel output rather than productive exchange.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "published": "2026-02-23T17:14:29Z",
    "updated": "2026-02-23T17:14:29Z",
    "url": "https://arxiv.org/abs/2602.20059v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20059v1",
    "relevance_score": 10.0,
    "display_category": "LLMs & Agents",
    "summary": "As multi-agent architectures and agent-to-agent protocols proliferate, a fundamental question arises: what actually happens when autonomous LLM agents interact at scale? We study this question...",
    "keyword_matches": [
      "multi-agent"
    ]
  },
  {
    "arxiv_id": "2602.20157v1",
    "title": "Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning",
    "authors": [
      "Zhongxiao Cong",
      "Qitao Zhao",
      "Minsik Jeon",
      "Shubham Tulsiani"
    ],
    "abstract": "Current feed-forward 3D/4D reconstruction systems rely on dense geometry and pose supervision -- expensive to obtain at scale and particularly scarce for dynamic real-world scenes. We present Flow3r, a framework that augments visual geometry learning with dense 2D correspondences (`flow') as supervision, enabling scalable training from unlabeled monocular videos. Our key insight is that the flow prediction module should be factored: predicting flow between two images using geometry latents from one and pose latents from the other. This factorization directly guides the learning of both scene geometry and camera motion, and naturally extends to dynamic scenes. In controlled experiments, we show that factored flow prediction outperforms alternative designs and that performance scales consistently with unlabeled data. Integrating factored flow into existing visual geometry architectures and training with ${\\sim}800$K unlabeled videos, Flow3r achieves state-of-the-art results across eight benchmarks spanning static and dynamic scenes, with its largest gains on in-the-wild dynamic videos where labeled data is most scarce.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "published": "2026-02-23T18:59:30Z",
    "updated": "2026-02-23T18:59:30Z",
    "url": "https://arxiv.org/abs/2602.20157v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20157v1",
    "relevance_score": 0.0,
    "display_category": "Computer Vision",
    "summary": "Current feed-forward 3D/4D reconstruction systems rely on dense geometry and pose supervision -- expensive to obtain at scale and particularly scarce for dynamic real-world scenes.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20156v1",
    "title": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks",
    "authors": [
      "David Schmotz",
      "Luca Beurer-Kellner",
      "Sahar Abdelnabi",
      "Maksym Andriushchenko"
    ],
    "abstract": "LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "published": "2026-02-23T18:59:27Z",
    "updated": "2026-02-23T18:59:27Z",
    "url": "https://arxiv.org/abs/2602.20156v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20156v1",
    "relevance_score": 0.0,
    "display_category": "LLMs & Agents",
    "summary": "LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20153v1",
    "title": "JUCAL: Jointly Calibrating Aleatoric and Epistemic Uncertainty in Classification Tasks",
    "authors": [
      "Jakob Heiss",
      "Sören Lambrecht",
      "Jakob Weissteiner",
      "Hanna Wutte",
      "Žan Žurič",
      "Josef Teichmann",
      "Bin Yu"
    ],
    "abstract": "We study post-calibration uncertainty for trained ensembles of classifiers. Specifically, we consider both aleatoric (label noise) and epistemic (model) uncertainty. Among the most popular and widely used calibration methods in classification are temperature scaling (i.e., pool-then-calibrate) and conformal methods. However, the main shortcoming of these calibration methods is that they do not balance the proportion of aleatoric and epistemic uncertainty. Not balancing these uncertainties can severely misrepresent predictive uncertainty, leading to overconfident predictions in some input regions while being underconfident in others. To address this shortcoming, we present a simple but powerful calibration algorithm Joint Uncertainty Calibration (JUCAL) that jointly calibrates aleatoric and epistemic uncertainty. JUCAL jointly calibrates two constants to weight and scale epistemic and aleatoric uncertainties by optimizing the negative log-likelihood (NLL) on the validation/calibration dataset. JUCAL can be applied to any trained ensemble of classifiers (e.g., transformers, CNNs, or tree-based methods), with minimal computational overhead, without requiring access to the models' internal parameters. We experimentally evaluate JUCAL on various text classification tasks, for ensembles of varying sizes and with different ensembling strategies. Our experiments show that JUCAL significantly outperforms SOTA calibration methods across all considered classification tasks, reducing NLL and predictive set size by up to 15% and 20%, respectively. Interestingly, even applying JUCAL to an ensemble of size 5 can outperform temperature-scaled ensembles of size up to 50 in terms of NLL and predictive set size, resulting in up to 10 times smaller inference costs. Thus, we propose JUCAL as a new go-to method for calibrating ensembles in classification.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "published": "2026-02-23T18:59:10Z",
    "updated": "2026-02-23T18:59:10Z",
    "url": "https://arxiv.org/abs/2602.20153v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20153v1",
    "relevance_score": 0.0,
    "display_category": "Machine Learning & Deep Learning",
    "summary": "We study post-calibration uncertainty for trained ensembles of classifiers.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20152v1",
    "title": "Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data",
    "authors": [
      "Zhenyao Ma",
      "Yue Liang",
      "Dongxu Li"
    ],
    "abstract": "Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data, ranging from single optimization problems to hierarchical compositions. It unifies predictive performance, intrinsic interpretability, and identifiability, with broad applicability to scientific domains involving optimization. BL parameterizes a compositional utility function built from intrinsically interpretable modular blocks, which induces a data distribution for prediction and generation. Each block represents and can be written in symbolic form as a utility maximization problem (UMP), a foundational paradigm in behavioral science and a universal framework of optimization. BL supports architectures ranging from a single UMP to hierarchical compositions, the latter modeling hierarchical optimization structures. Its smooth and monotone variant (IBL) guarantees identifiability. Theoretically, we establish the universal approximation property of BL, and analyze the M-estimation properties of IBL. Empirically, BL demonstrates strong predictive performance, intrinsic interpretability and scalability to high-dimensional data. Code: https://github.com/MoonYLiang/Behavior-Learning ; install via pip install blnetwork.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2026-02-23T18:59:04Z",
    "updated": "2026-02-23T18:59:04Z",
    "url": "https://arxiv.org/abs/2602.20152v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20152v1",
    "relevance_score": 0.0,
    "display_category": "Machine Learning & Deep Learning",
    "summary": "Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data,...",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20137v1",
    "title": "Do Large Language Models Understand Data Visualization Rules?",
    "authors": [
      "Martin Sinnona",
      "Valentin Bonas",
      "Emmanuel Iarussi",
      "Viviana Siless"
    ],
    "abstract": "Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they can reason about and enforce visualization rules directly. Constraint-based systems such as Draco encode these rules as logical constraints for precise automated checks, but maintaining symbolic encodings requires expert effort, motivating the use of LLMs as flexible rule validators. In this paper, we present the first systematic evaluation of LLMs against visualization rules using hard-verification ground truth derived from Answer Set Programming (ASP). We translated a subset of Draco's constraints into natural-language statements and generated a controlled dataset of 2,000 Vega-Lite specifications annotated with explicit rule violations. LLMs were evaluated on both accuracy in detecting violations and prompt adherence, which measures whether outputs follow the required structured format. Results show that frontier models achieve high adherence (Gemma 3 4B / 27B: 100%, GPT-oss 20B: 98%) and reliably detect common violations (F1 up to 0.82),yet performance drops for subtler perceptual rules (F1 < 0.15 for some categories) and for outputs generated from technical ASP formulations.Translating constraints into natural language improved performance by up to 150% for smaller models. These findings demonstrate the potential of LLMs as flexible, language-driven validators while highlighting their current limitations compared to symbolic solvers.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "published": "2026-02-23T18:47:51Z",
    "updated": "2026-02-23T18:47:51Z",
    "url": "https://arxiv.org/abs/2602.20137v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20137v1",
    "relevance_score": 0.0,
    "display_category": "Computer Vision",
    "summary": "Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20134v1",
    "title": "Modeling Epidemiological Dynamics Under Adversarial Data and User Deception",
    "authors": [
      "Yiqi Su",
      "Christo Kurisummoottil Thomas",
      "Walid Saad",
      "Bud Mishra",
      "Naren Ramakrishnan"
    ],
    "abstract": "Epidemiological models increasingly rely on self-reported behavioral data such as vaccination status, mask usage, and social distancing adherence to forecast disease transmission and assess the impact of non-pharmaceutical interventions (NPIs). While such data provide valuable real-time insights, they are often subject to strategic misreporting, driven by individual incentives to avoid penalties, access benefits, or express distrust in public health authorities. To account for such human behavior, in this paper, we introduce a game-theoretic framework that models the interaction between the population and a public health authority as a signaling game. Individuals (senders) choose how to report their behaviors, while the public health authority (receiver) updates their epidemiological model(s) based on potentially distorted signals. Focusing on deception around masking and vaccination, we characterize analytically game equilibrium outcomes and evaluate the degree to which deception can be tolerated while maintaining epidemic control through policy interventions. Our results show that separating equilibria-with minimal deception-drive infections to near zero over time. Remarkably, even under pervasive dishonesty in pooling equilibria, well-designed sender and receiver strategies can still maintain effective epidemic control. This work advances the understanding of adversarial data in epidemiology and offers tools for designing more robust public health models in the presence of strategic user behavior.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "published": "2026-02-23T18:45:55Z",
    "updated": "2026-02-23T18:45:55Z",
    "url": "https://arxiv.org/abs/2602.20134v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20134v1",
    "relevance_score": 0.0,
    "display_category": "Machine Learning & Deep Learning",
    "summary": "Epidemiological models increasingly rely on self-reported behavioral data such as vaccination status, mask usage, and social distancing adherence to forecast disease transmission and assess the...",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20133v1",
    "title": "AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization",
    "authors": [
      "Mert Cemri",
      "Shubham Agrawal",
      "Akshat Gupta",
      "Shu Liu",
      "Audrey Cheng",
      "Qiuyang Mang",
      "Ashwin Naren",
      "Lutfi Eren Erdogan",
      "Koushik Sen",
      "Matei Zaharia",
      "Alex Dimakis",
      "Ion Stoica"
    ],
    "abstract": "The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promising frontiers remain under-exploited. We introduce AdaEvolve, a framework that reformulates LLM-driven evolution as a hierarchical adaptive optimization problem. AdaEvolve uses an \"accumulated improvement signal\" to unify decisions across three levels: Local Adaptation, which dynamically modulates the exploration intensity within a population of solution candidates; Global Adaptation, which routes the global resource budget via bandit-based scheduling across different solution candidate populations; and Meta-Guidance which generates novel solution tactics based on the previously generated solutions and their corresponding improvements when the progress stalls. We demonstrate that AdaEvolve consistently outperforms the open-sourced baselines across 185 different open-ended optimization problems including combinatorial, systems optimization and algorithm design problems.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.NE",
    "published": "2026-02-23T18:45:31Z",
    "updated": "2026-02-23T18:45:31Z",
    "url": "https://arxiv.org/abs/2602.20133v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20133v1",
    "relevance_score": 0.0,
    "display_category": "LLMs & Agents",
    "summary": "The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within...",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20132v1",
    "title": "LAD: Learning Advantage Distribution for Reasoning",
    "authors": [
      "Wendi Li",
      "Sharon Li"
    ],
    "abstract": "Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishing the equivalence between the optimal policy update and an advantage-based target distribution, we derive a practical LAD objective formulated as minimizing an $f$-divergence between the policy-induced and advantage-induced distributions. This yields a gradient update that increases likelihood for high-advantage responses while suppressing over-confident probability growth, preventing collapse without requiring auxiliary entropy regularization. LAD incurs no extra training cost compared to GRPO and scales naturally to LLM post-training. In a controlled bandit setting, LAD faithfully recovers the multimodal advantage distribution, validating the theoretical formulation. Experiments on math and code reasoning tasks across several LLM backbones show that LAD reliably improves both accuracy and generative diversity.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2026-02-23T18:44:10Z",
    "updated": "2026-02-23T18:44:10Z",
    "url": "https://arxiv.org/abs/2602.20132v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20132v1",
    "relevance_score": 0.0,
    "display_category": "Machine Learning & Deep Learning",
    "summary": "Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20130v1",
    "title": "To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering",
    "authors": [
      "Zaifu Zhan",
      "Min Zeng",
      "Shuang Zhou",
      "Yiran Song",
      "Xiaoyi Chen",
      "Yu Hou",
      "Yifan Wu",
      "Yang Ruan",
      "Rui Zhang"
    ],
    "abstract": "Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy. Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and PubMedQA. Metrics included accuracy, total generated tokens, and inference time. Results: Selective CoT reduced inference time by 13-45% and token usage by 8-47% with minimal accuracy loss ($\\leq$4\\%). In some model-task pairs, it achieved both higher accuracy and greater efficiency than standard CoT. Compared with fixed-length CoT, Selective CoT reached similar or superior accuracy at substantially lower computational cost. Discussion: Selective CoT dynamically balances reasoning depth and efficiency by invoking explicit reasoning only when beneficial, reducing redundancy on recall-type questions while preserving interpretability. Conclusion: Selective CoT provides a simple, model-agnostic, and cost-effective approach for medical QA, aligning reasoning effort with question complexity to enhance real-world deployability of LLM-based clinical systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2026-02-23T18:42:50Z",
    "updated": "2026-02-23T18:42:50Z",
    "url": "https://arxiv.org/abs/2602.20130v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20130v1",
    "relevance_score": 0.0,
    "display_category": "LLMs & Agents",
    "summary": "Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20126v1",
    "title": "Adaptation to Intrinsic Dependence in Diffusion Language Models",
    "authors": [
      "Yunxiao Zhao",
      "Changxiao Cai"
    ],
    "abstract": "Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) approaches, enabling parallel token generation beyond a rigid left-to-right order. Despite growing empirical success, the theoretical understanding of how unmasking schedules -- which specify the order and size of unmasked tokens during sampling -- affect generation quality remains limited. In this work, we introduce a distribution-agnostic unmasking schedule for DLMs that adapts to the (unknown) dependence structure of the target data distribution, without requiring any prior knowledge or hyperparameter tuning. In contrast to prior deterministic procedures that fix unmasking sizes, our method randomizes the number of tokens revealed at each iteration. We show that, for two specific parameter choices, the sampling convergence guarantees -- measured by Kullback-Leibler (KL) divergence -- scale as $\\widetilde O(\\mathsf{TC}/K)$ and $\\widetilde O(\\mathsf{DTC}/K)$ respectively. Here, $K$ is the number of iterations, and $\\mathsf{TC}$ and $\\mathsf{DTC}$ are the total correlation and dual total correlation of the target distribution, capturing the intrinsic dependence structure underlying the data. Importantly, our guarantees hold in the practically relevant parallel-sampling regime $K<L$ where $L$ is the token sequence length. These results significantly improve upon prior convergence theories and yield substantial sampling acceleration for low-complexity distributions. Overall, our findings unveil the adaptivity of DLMs to intrinsic data structures and shed light on the benefit of randomized unmasking sizes in inference schedule design.",
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2026-02-23T18:41:34Z",
    "updated": "2026-02-23T18:41:34Z",
    "url": "https://arxiv.org/abs/2602.20126v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20126v1",
    "relevance_score": 0.0,
    "display_category": "Machine Learning & Deep Learning",
    "summary": "Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) approaches, enabling parallel token generation beyond a rigid left-to-right order.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20122v1",
    "title": "NanoKnow: How to Know What Your Language Model Knows",
    "authors": [
      "Lingwei Gu",
      "Nour Jedidi",
      "Jimmy Lin"
    ],
    "abstract": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "published": "2026-02-23T18:37:49Z",
    "updated": "2026-02-23T18:37:49Z",
    "url": "https://arxiv.org/abs/2602.20122v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20122v1",
    "relevance_score": 0.0,
    "display_category": "LLMs & Agents",
    "summary": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20119v1",
    "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning",
    "authors": [
      "Jiahui Fu",
      "Junyu Nan",
      "Lingfeng Sun",
      "Hongyu Li",
      "Jianing Qian",
      "Jennifer L. Barry",
      "Kris Kitani",
      "George Konidaris"
    ],
    "abstract": "Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "published": "2026-02-23T18:35:18Z",
    "updated": "2026-02-23T18:35:18Z",
    "url": "https://arxiv.org/abs/2602.20119v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20119v1",
    "relevance_score": 0.0,
    "display_category": "Computer Vision",
    "summary": "Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20113v1",
    "title": "StyleStream: Real-Time Zero-Shot Voice Style Conversion",
    "authors": [
      "Yisi Liu",
      "Nicholas Lee",
      "Gopala Anumanchipalli"
    ],
    "abstract": "Voice style conversion aims to transform an input utterance to match a target speaker's timbre, accent, and emotion, with a central challenge being the disentanglement of linguistic content from style. While prior work has explored this problem, conversion quality remains limited, and real-time voice style conversion has not been addressed. We propose StyleStream, the first streamable zero-shot voice style conversion system that achieves state-of-the-art performance. StyleStream consists of two components: a Destylizer, which removes style attributes while preserving linguistic content, and a Stylizer, a diffusion transformer (DiT) that reintroduces target style conditioned on reference speech. Robust content-style disentanglement is enforced through text supervision and a highly constrained information bottleneck. This design enables a fully non-autoregressive architecture, achieving real-time voice style conversion with an end-to-end latency of 1 second. Samples and real-time demo: https://berkeley-speech-group.github.io/StyleStream/.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "published": "2026-02-23T18:32:59Z",
    "updated": "2026-02-23T18:32:59Z",
    "url": "https://arxiv.org/abs/2602.20113v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20113v1",
    "relevance_score": 0.0,
    "display_category": "LLMs & Agents",
    "summary": "Voice style conversion aims to transform an input utterance to match a target speaker's timbre, accent, and emotion, with a central challenge being the disentanglement of linguistic content from...",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20111v1",
    "title": "Reliable Abstention under Adversarial Injections: Tight Lower Bounds and New Upper Bounds",
    "authors": [
      "Ezra Edelman",
      "Surbhi Goel"
    ],
    "abstract": "We study online learning in the adversarial injection model introduced by [Goel et al. 2017], where a stream of labeled examples is predominantly drawn i.i.d.\\ from an unknown distribution $\\mathcal{D}$, but may be interspersed with adversarially chosen instances without the learner knowing which rounds are adversarial. Crucially, labels are always consistent with a fixed target concept (the clean-label setting). The learner is additionally allowed to abstain from predicting, and the total error counts the mistakes whenever the learner decides to predict and incorrect abstentions when it abstains on i.i.d.\\ rounds. Perhaps surprisingly, prior work shows that oracle access to the underlying distribution yields $O(d^2 \\log T)$ combined error for VC dimension $d$, while distribution-agnostic algorithms achieve only $\\tilde{O}(\\sqrt{T})$ for restricted classes, leaving open whether this gap is fundamental. We resolve this question by proving a matching $Ω(\\sqrt{T})$ lower bound for VC dimension $1$, establishing a sharp separation between the two information regimes. On the algorithmic side, we introduce a potential-based framework driven by \\emph{robust witnesses}, small subsets of labeled examples that certify predictions while remaining resilient to adversarial contamination. We instantiate this framework using two combinatorial dimensions: (1) \\emph{inference dimension}, yielding combined error $\\tilde{O}(T^{1-1/k})$ for classes of inference dimension $k$, and (2) \\emph{certificate dimension}, a new relaxation we introduce. As an application, we show that halfspaces in $\\mathbb{R}^2$ have certificate dimension $3$, obtaining the first distribution-agnostic bound of $\\tilde{O}(T^{2/3})$ for this class. This is notable since [Blum et al. 2021] showed halfspaces are not robustly learnable under clean-label attacks without abstention.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2026-02-23T18:30:48Z",
    "updated": "2026-02-23T18:30:48Z",
    "url": "https://arxiv.org/abs/2602.20111v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20111v1",
    "relevance_score": 0.0,
    "display_category": "Machine Learning & Deep Learning",
    "summary": "We study online learning in the adversarial injection model introduced by [Goel et al.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20104v1",
    "title": "Align When They Want, Complement When They Need! Human-Centered Ensembles for Adaptive Human-AI Collaboration",
    "authors": [
      "Hasan Amin",
      "Ming Yin",
      "Rajiv Khanna"
    ],
    "abstract": "In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in areas of human strengths. This can inadvertently erode human trust and cause them to ignore AI advice precisely when it is most needed. Conversely, an aligned AI fosters trust yet risks reinforcing suboptimal human behavior and lowering human-AI team performance. In this paper, we start by identifying this fundamental tension between performance-boosting (i.e., complementarity) and trust-building (i.e., alignment) as an inherent limitation of the traditional approach for training a single AI model to assist human decision making. To overcome this, we introduce a novel human-centered adaptive AI ensemble that strategically toggles between two specialist AI models - the aligned model and the complementary model - based on contextual cues, using an elegantly simple yet provably near-optimal Rational Routing Shortcut mechanism. Comprehensive theoretical analyses elucidate why the adaptive AI ensemble is effective and when it yields maximum benefits. Moreover, experiments on both simulated and real-world data show that when humans are assisted by the adaptive AI ensemble in decision making, they can achieve significantly higher performance than when they are assisted by single AI models that are trained to either optimize for their independent performance or even the human-AI team performance.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "published": "2026-02-23T18:22:58Z",
    "updated": "2026-02-23T18:22:58Z",
    "url": "https://arxiv.org/abs/2602.20104v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20104v1",
    "relevance_score": 0.0,
    "display_category": "Machine Learning & Deep Learning",
    "summary": "In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance...",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20102v1",
    "title": "BarrierSteer: LLM Safety via Learning Barrier Steering",
    "authors": [
      "Thanh Q. Tran",
      "Arun Verma",
      "Kiwan Wong",
      "Bryan Kian Hsiang Low",
      "Daniela Rus",
      "Wei Xiao"
    ],
    "abstract": "Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings. Addressing this challenge requires safety mechanisms that are both practically effective and supported by rigorous theory. We introduce BarrierSteer, a novel framework that formalizes response safety by embedding learned non-linear safety constraints directly into the model's latent representation space. BarrierSteer employs a steering mechanism based on Control Barrier Functions (CBFs) to efficiently detect and prevent unsafe response trajectories during inference with high precision. By enforcing multiple safety constraints through efficient constraint merging, without modifying the underlying LLM parameters, BarrierSteer preserves the model's original capabilities and performance. We provide theoretical results establishing that applying CBFs in latent space offers a principled and computationally efficient approach to enforcing safety. Our experiments across multiple models and datasets show that BarrierSteer substantially reduces adversarial success rates, decreases unsafe generations, and outperforms existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "published": "2026-02-23T18:19:46Z",
    "updated": "2026-02-23T18:19:46Z",
    "url": "https://arxiv.org/abs/2602.20102v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20102v1",
    "relevance_score": 0.0,
    "display_category": "Machine Learning & Deep Learning",
    "summary": "Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to...",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20100v1",
    "title": "Transcending the Annotation Bottleneck: AI-Powered Discovery in Biology and Medicine",
    "authors": [
      "Soumick Chatterjee"
    ],
    "abstract": "The dependence on expert annotation has long constituted the primary rate-limiting step in the application of artificial intelligence to biomedicine. While supervised learning drove the initial wave of clinical algorithms, a paradigm shift towards unsupervised and self-supervised learning (SSL) is currently unlocking the latent potential of biobank-scale datasets. By learning directly from the intrinsic structure of data - whether pixels in a magnetic resonance image (MRI), voxels in a volumetric scan, or tokens in a genomic sequence - these methods facilitate the discovery of novel phenotypes, the linkage of morphology to genetics, and the detection of anomalies without human bias. This article synthesises seminal and recent advances in \"learning without labels,\" highlighting how unsupervised frameworks can derive heritable cardiac traits, predict spatial gene expression in histology, and detect pathologies with performance that rivals or exceeds supervised counterparts.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "published": "2026-02-23T18:15:30Z",
    "updated": "2026-02-23T18:15:30Z",
    "url": "https://arxiv.org/abs/2602.20100v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20100v1",
    "relevance_score": 0.0,
    "display_category": "Computer Vision",
    "summary": "The dependence on expert annotation has long constituted the primary rate-limiting step in the application of artificial intelligence to biomedicine.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20093v1",
    "title": "ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation",
    "authors": [
      "Kun Yang",
      "Yuxuan Zhu",
      "Yazhe Chen",
      "Siyao Zheng",
      "Bangyang Hong",
      "Kangle Wu",
      "Yabo Ni",
      "Anxiang Zeng",
      "Cong Fu",
      "Hui Li"
    ],
    "abstract": "Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computation. Despite empirical gains, existing approaches largely drive intermediate reasoning states via target-dominant objectives without imposing explicit feasibility constraints. This results in latent drift, where reasoning trajectories deviate into implausible regions. We argue that effective recommendation reasoning should instead be viewed as navigation on a collaborative manifold rather than free-form latent refinement. To this end, we propose ManCAR (Manifold-Constrained Adaptive Reasoning), a principled framework that grounds reasoning within the topology of a global interaction graph. ManCAR constructs a local intent prior from the collaborative neighborhood of a user's recent actions, represented as a distribution over the item simplex. During training, the model progressively aligns its latent predictive distribution with this prior, forcing the reasoning trajectory to remain within the valid manifold. At test time, reasoning proceeds adaptively until the predictive distribution stabilizes, avoiding over-refinement. We provide a variational interpretation of ManCAR to theoretically validate its drift-prevention and adaptive test-time stopping mechanisms. Experiments on seven benchmarks demonstrate that ManCAR consistently outperforms state-of-the-art baselines, achieving up to a 46.88% relative improvement w.r.t. NDCG@10. Our code is available at https://github.com/FuCongResearchSquad/ManCAR.",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR",
    "published": "2026-02-23T18:02:50Z",
    "updated": "2026-02-23T18:02:50Z",
    "url": "https://arxiv.org/abs/2602.20093v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20093v1",
    "relevance_score": 0.0,
    "display_category": "Retrieval & RAG",
    "summary": "Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computation.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20092v1",
    "title": "BabyLM Turns 4: Call for Papers for the 2026 BabyLM Workshop",
    "authors": [
      "Leshem Choshen",
      "Ryan Cotterell",
      "Mustafa Omer Gul",
      "Jaap Jumelet",
      "Tal Linzen",
      "Aaron Mueller",
      "Suchir Salhan",
      "Raj Sanjay Shah",
      "Alex Warstadt",
      "Ethan Gotlieb Wilcox"
    ],
    "abstract": "BabyLM aims to dissolve the boundaries between cognitive modeling and language modeling. We call for both workshop papers and for researchers to join the 4th BabyLM competition. As in previous years, we call for participants in the data-efficient pretraining challenge in the general track. This year, we also offer a new track: Multilingual. We also call for papers outside the competition in any relevant areas. These include training efficiency, cognitively plausible research, weak model evaluation, and more.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2026-02-23T18:02:23Z",
    "updated": "2026-02-23T18:02:23Z",
    "url": "https://arxiv.org/abs/2602.20092v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20092v1",
    "relevance_score": 0.0,
    "display_category": "LLMs & Agents",
    "summary": "BabyLM aims to dissolve the boundaries between cognitive modeling and language modeling.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20089v1",
    "title": "StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues",
    "authors": [
      "Zanxi Ruan",
      "Qiuyu Kong",
      "Songqun Gao",
      "Yiming Wang",
      "Marco Cristani"
    ],
    "abstract": "Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval. We introduce StructXLIP, a fine-tuning alignment paradigm that extracts edge maps (e.g., Canny), treating them as proxies for the visual structure of an image, and filters the corresponding captions to emphasize structural cues, making them \"structure-centric\". Fine-tuning augments the standard alignment loss with three structure-centric losses: (i) aligning edge maps with structural text, (ii) matching local edge regions to textual chunks, and (iii) connecting edge maps to color images to prevent representation drift. From a theoretical standpoint, while standard CLIP maximizes the mutual information between visual and textual embeddings, StructXLIP additionally maximizes the mutual information between multimodal structural representations. This auxiliary optimization is intrinsically harder, guiding the model toward more robust and semantically stable minima, enhancing vision-language alignment. Beyond outperforming current competitors on cross-modal retrieval in both general and specialized domains, our method serves as a general boosting recipe that can be integrated into future approaches in a plug-and-play manner. Code and pretrained models are publicly available at: https://github.com/intelligolabs/StructXLIP.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "published": "2026-02-23T17:57:37Z",
    "updated": "2026-02-23T17:57:37Z",
    "url": "https://arxiv.org/abs/2602.20089v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20089v1",
    "relevance_score": 0.0,
    "display_category": "Computer Vision",
    "summary": "Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20079v1",
    "title": "SemanticNVS: Improving Semantic Scene Understanding in Generative Novel View Synthesis",
    "authors": [
      "Xinya Chen",
      "Christopher Wewer",
      "Jiahao Xie",
      "Xinting Hu",
      "Jan Eric Lenssen"
    ],
    "abstract": "We present SemanticNVS, a camera-conditioned multi-view diffusion model for novel view synthesis (NVS), which improves generation quality and consistency by integrating pre-trained semantic feature extractors. Existing NVS methods perform well for views near the input view, however, they tend to generate semantically implausible and distorted images under long-range camera motion, revealing severe degradation. We speculate that this degradation is due to current models failing to fully understand their conditioning or intermediate generated scene content. Here, we propose to integrate pre-trained semantic feature extractors to incorporate stronger scene semantics as conditioning to achieve high-quality generation even at distant viewpoints. We investigate two different strategies, (1) warped semantic features and (2) an alternating scheme of understanding and generation at each denoising step. Experimental results on multiple datasets demonstrate the clear qualitative and quantitative (4.69%-15.26% in FID) improvement over state-of-the-art alternatives.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "published": "2026-02-23T17:45:21Z",
    "updated": "2026-02-23T17:45:21Z",
    "url": "https://arxiv.org/abs/2602.20079v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20079v1",
    "relevance_score": 0.0,
    "display_category": "Computer Vision",
    "summary": "We present SemanticNVS, a camera-conditioned multi-view diffusion model for novel view synthesis (NVS), which improves generation quality and consistency by integrating pre-trained semantic...",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20076v1",
    "title": "Robust Taylor-Lagrange Control for Safety-Critical Systems",
    "authors": [
      "Wei Xiao",
      "Christos Cassandras",
      "Anni Li"
    ],
    "abstract": "Solving safety-critical control problem has widely adopted the Control Barrier Function (CBF) method. However, the existence of a CBF is only a sufficient condition for system safety. The recently proposed Taylor-Lagrange Control (TLC) method addresses this limitation, but is vulnerable to the feasibility preservation problem (e.g., inter-sampling effect). In this paper, we propose a robust TLC (rTLC) method to address the feasibility preservation problem. Specifically, the rTLC method expands the safety function at an order higher than the relative degree of the function using Taylor's expansion with Lagrange remainder, which allows the control to explicitly show up at the current time instead of the future time in the TLC method. The rTLC method naturally addresses the feasibility preservation problem with only one hyper-parameter (the discretization time interval size during implementation), which is much less than its counterparts. Finally, we illustrate the effectiveness of the proposed rTLC method through an adaptive cruise control problem, and compare it with existing safety-critical control methods.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "eess.SY",
    "published": "2026-02-23T17:40:05Z",
    "updated": "2026-02-23T17:40:05Z",
    "url": "https://arxiv.org/abs/2602.20076v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20076v1",
    "relevance_score": 0.0,
    "display_category": "Machine Learning & Deep Learning",
    "summary": "Solving safety-critical control problem has widely adopted the Control Barrier Function (CBF) method.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20070v1",
    "title": "Training-Free Generative Modeling via Kernelized Stochastic Interpolants",
    "authors": [
      "Florentin Coeurdoux",
      "Etienne Lempereur",
      "Nathanaël Cuvelle-Magar",
      "Thomas Eboli",
      "Stéphane Mallat",
      "Anastasia Borovykh",
      "Eric Vanden-Eijnden"
    ],
    "abstract": "We develop a kernel method for generative modeling within the stochastic interpolant framework, replacing neural network training with linear systems. The drift of the generative SDE is $\\hat b_t(x) = \\nablaφ(x)^\\topη_t$, where $η_t\\in\\R^P$ solves a $P\\times P$ system computable from data, with $P$ independent of the data dimension $d$. Since estimates are inexact, the diffusion coefficient $D_t$ affects sample quality; the optimal $D_t^*$ from Girsanov diverges at $t=0$, but this poses no difficulty and we develop an integrator that handles it seamlessly. The framework accommodates diverse feature maps -- scattering transforms, pretrained generative models etc. -- enabling training-free generation and model combination. We demonstrate the approach on financial time series, turbulence, and image generation.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "published": "2026-02-23T17:26:09Z",
    "updated": "2026-02-23T17:26:09Z",
    "url": "https://arxiv.org/abs/2602.20070v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20070v1",
    "relevance_score": 0.0,
    "display_category": "Machine Learning & Deep Learning",
    "summary": "We develop a kernel method for generative modeling within the stochastic interpolant framework, replacing neural network training with linear systems.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20069v1",
    "title": "Input/output coloring and Gröbner basis for dioperads",
    "authors": [
      "Anton Khoroshkin"
    ],
    "abstract": "By selecting a specific input or output of a dioperadic tree, we transform it into a rooted tree and induce a corresponding colored operadic structure. This fundamental pictorial construction demonstrates how the machinery of Gröbner bases and the theory of Hilbert series (well-established for (colored) operads) can be adapted to the dioperadic setting. We illustrate this framework by providing several examples and applications: (1) we compute the dimensions of the spaces of operations for the dioperad of Lie bialgebras; (2) we describe a Gröbner basis and a minimal resolution for the dioperad of triangular Lie bialgebras; (3) we provide computations for the dioperad of ``algebraic string operations''; (4) we present a graphical construction that establishes the existence of quadratic Gröbner bases and the Koszul property for a broad class of dioperads originating from cyclic operads.",
    "categories": [
      "math.QA",
      "math.AT",
      "math.CO"
    ],
    "primary_category": "math.QA",
    "published": "2026-02-23T17:25:49Z",
    "updated": "2026-02-23T17:25:49Z",
    "url": "https://arxiv.org/abs/2602.20069v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20069v1",
    "relevance_score": 0.0,
    "display_category": "Topological Data Analysis",
    "summary": "By selecting a specific input or output of a dioperadic tree, we transform it into a rooted tree and induce a corresponding colored operadic structure.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20068v1",
    "title": "The Invisible Gorilla Effect in Out-of-distribution Detection",
    "authors": [
      "Harry Anthony",
      "Ziyun Liang",
      "Hermione Warr",
      "Konstantinos Kamnitsas"
    ],
    "abstract": "Deep Neural Networks achieve high performance in vision tasks by learning features from regions of interest (ROI) within images, but their performance degrades when deployed on out-of-distribution (OOD) data that differs from training data. This challenge has led to OOD detection methods that aim to identify and reject unreliable predictions. Although prior work shows that OOD detection performance varies by artefact type, the underlying causes remain underexplored. To this end, we identify a previously unreported bias in OOD detection: for hard-to-detect artefacts (near-OOD), detection performance typically improves when the artefact shares visual similarity (e.g. colour) with the model's ROI and drops when it does not - a phenomenon we term the Invisible Gorilla Effect. For example, in a skin lesion classifier with red lesion ROI, we show the method Mahalanobis Score achieves a 31.5% higher AUROC when detecting OOD red ink (similar to ROI) compared to black ink (dissimilar) annotations. We annotated artefacts by colour in 11,355 images from three public datasets (e.g. ISIC) and generated colour-swapped counterfactuals to rule out dataset bias. We then evaluated 40 OOD methods across 7 benchmarks and found significant performance drops for most methods when artefacts differed from the ROI. Our findings highlight an overlooked failure mode in OOD detection and provide guidance for more robust detectors. Code and annotations are available at: https://github.com/HarryAnthony/Invisible_Gorilla_Effect.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "published": "2026-02-23T17:24:18Z",
    "updated": "2026-02-23T17:24:18Z",
    "url": "https://arxiv.org/abs/2602.20068v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20068v1",
    "relevance_score": 0.0,
    "display_category": "Computer Vision",
    "summary": "Deep Neural Networks achieve high performance in vision tasks by learning features from regions of interest (ROI) within images, but their performance degrades when deployed on out-of-distribution...",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20066v1",
    "title": "HeatPrompt: Zero-Shot Vision-Language Modeling of Urban Heat Demand from Satellite Images",
    "authors": [
      "Kundan Thota",
      "Xuanhao Mu",
      "Thorsten Schlachter",
      "Veit Hagenmeyer"
    ],
    "abstract": "Accurate heat-demand maps play a crucial role in decarbonizing space heating, yet most municipalities lack detailed building-level data needed to calculate them. We introduce HeatPrompt, a zero-shot vision-language energy modeling framework that estimates annual heat demand using semantic features extracted from satellite images, basic Geographic Information System (GIS), and building-level features. We feed pretrained Large Vision Language Models (VLMs) with a domain-specific prompt to act as an energy planner and extract the visual attributes such as roof age, building density, etc, from the RGB satellite image that correspond to the thermal load. A Multi-Layer Perceptron (MLP) regressor trained on these captions shows an $R^2$ uplift of 93.7% and shrinks the mean absolute error (MAE) by 30% compared to the baseline model. Qualitative analysis shows that high-impact tokens align with high-demand zones, offering lightweight support for heat planning in data-scarce regions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "published": "2026-02-23T17:22:54Z",
    "updated": "2026-02-23T17:22:54Z",
    "url": "https://arxiv.org/abs/2602.20066v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20066v1",
    "relevance_score": 0.0,
    "display_category": "Computer Vision",
    "summary": "Accurate heat-demand maps play a crucial role in decarbonizing space heating, yet most municipalities lack detailed building-level data needed to calculate them.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20065v1",
    "title": "Multilingual Large Language Models do not comprehend all natural languages to equal degrees",
    "authors": [
      "Natalia Moskvina",
      "Raquel Montero",
      "Masaya Yoshida",
      "Ferdy Hubers",
      "Paolo Morosi",
      "Walid Irhaymi",
      "Jin Yan",
      "Tamara Serrano",
      "Elena Pagliarini",
      "Fritz Günther",
      "Evelina Leivada"
    ],
    "abstract": "Large Language Models (LLMs) play a critical role in how humans access information. While their core use relies on comprehending written requests, our understanding of this ability is currently limited, because most benchmarks evaluate LLMs in high-resource languages predominantly spoken by Western, Educated, Industrialised, Rich, and Democratic (WEIRD) communities. The default assumption is that English is the best-performing language for LLMs, while smaller, low-resource languages are linked to less reliable outputs, even in multilingual, state-of-the-art models. To track variation in the comprehension abilities of LLMs, we prompt 3 popular models on a language comprehension task across 12 languages, representing the Indo-European, Afro-Asiatic, Turkic, Sino-Tibetan, and Japonic language families. Our results suggest that the models exhibit remarkable linguistic accuracy across typologically diverse languages, yet they fall behind human baselines in all of them, albeit to different degrees. Contrary to what was expected, English is not the best-performing language, as it was systematically outperformed by several Romance languages, even lower-resource ones. We frame the results by discussing the role of several factors that drive LLM performance, such as tokenization, language distance from Spanish and English, size of training data, and data origin in high- vs. low-resource languages and WEIRD vs. non-WEIRD communities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "published": "2026-02-23T17:22:46Z",
    "updated": "2026-02-23T17:22:46Z",
    "url": "https://arxiv.org/abs/2602.20065v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20065v1",
    "relevance_score": 0.0,
    "display_category": "LLMs & Agents",
    "summary": "Large Language Models (LLMs) play a critical role in how humans access information.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20062v1",
    "title": "A Theory of How Pretraining Shapes Inductive Bias in Fine-Tuning",
    "authors": [
      "Nicolas Anguita",
      "Francesco Locatello",
      "Andrew M. Saxe",
      "Marco Mondelli",
      "Flavia Mancini",
      "Samuel Lippl",
      "Clementine Domine"
    ],
    "abstract": "Pretraining and fine-tuning are central stages in modern machine learning systems. In practice, feature learning plays an important role across both stages: deep neural networks learn a broad range of useful features during pretraining and further refine those features during fine-tuning. However, an end-to-end theoretical understanding of how choices of initialization impact the ability to reuse and refine features during fine-tuning has remained elusive. Here we develop an analytical theory of the pretraining-fine-tuning pipeline in diagonal linear networks, deriving exact expressions for the generalization error as a function of initialization parameters and task statistics. We find that different initialization choices place the network into four distinct fine-tuning regimes that are distinguished by their ability to support feature learning and reuse, and therefore by the task statistics for which they are beneficial. In particular, a smaller initialization scale in earlier layers enables the network to both reuse and refine its features, leading to superior generalization on fine-tuning tasks that rely on a subset of pretraining features. We demonstrate empirically that the same initialization parameters impact generalization in nonlinear networks trained on CIFAR-100. Overall, our results demonstrate analytically how data and network initialization interact to shape fine-tuning generalization, highlighting an important role for the relative scale of initialization across different layers in enabling continued feature learning during fine-tuning.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "published": "2026-02-23T17:19:33Z",
    "updated": "2026-02-23T17:19:33Z",
    "url": "https://arxiv.org/abs/2602.20062v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20062v1",
    "relevance_score": 0.0,
    "display_category": "Machine Learning & Deep Learning",
    "summary": "Pretraining and fine-tuning are central stages in modern machine learning systems.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20060v1",
    "title": "MeanFuser: Fast One-Step Multi-Modal Trajectory Generation and Adaptive Reconstruction via MeanFlow for End-to-End Autonomous Driving",
    "authors": [
      "Junli Wang",
      "Xueyi Liu",
      "Yinan Zheng",
      "Zebing Xing",
      "Pengfei Li",
      "Guang Li",
      "Kun Ma",
      "Guang Chen",
      "Hangjun Ye",
      "Zhongpu Xia",
      "Long Chen",
      "Qichao Zhang"
    ],
    "abstract": "Generative models have shown great potential in trajectory planning. Recent studies demonstrate that anchor-guided generative models are effective in modeling the uncertainty of driving behaviors and improving overall performance. However, these methods rely on discrete anchor vocabularies that must sufficiently cover the trajectory distribution during testing to ensure robustness, inducing an inherent trade-off between vocabulary size and model performance. To overcome this limitation, we propose MeanFuser, an end-to-end autonomous driving method that enhances both efficiency and robustness through three key designs. (1) We introduce Gaussian Mixture Noise (GMN) to guide generative sampling, enabling a continuous representation of the trajectory space and eliminating the dependency on discrete anchor vocabularies. (2) We adapt ``MeanFlow Identity\" to end-to-end planning, which models the mean velocity field between GMN and trajectory distribution instead of the instantaneous velocity field used in vanilla flow matching methods, effectively eliminating numerical errors from ODE solvers and significantly accelerating inference. (3) We design a lightweight Adaptive Reconstruction Module (ARM) that enables the model to implicitly select from all sampled proposals or reconstruct a new trajectory when none is satisfactory via attention weights. Experiments on the NAVSIM closed-loop benchmark demonstrate that MeanFuser achieves outstanding performance without the supervision of the PDM Score. and exceptional inference efficiency, offering a robust and efficient solution for end-to-end autonomous driving. Our code and model are available at https://github.com/wjl2244/MeanFuser.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "published": "2026-02-23T17:17:26Z",
    "updated": "2026-02-23T17:17:26Z",
    "url": "https://arxiv.org/abs/2602.20060v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20060v1",
    "relevance_score": 0.0,
    "display_category": "Computer Vision",
    "summary": "Generative models have shown great potential in trajectory planning.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20057v1",
    "title": "AdaWorldPolicy: World-Model-Driven Diffusion Policy with Online Adaptive Learning for Robotic Manipulation",
    "authors": [
      "Ge Yuan",
      "Qiyuan Qiao",
      "Jing Zhang",
      "Dong Xu"
    ],
    "abstract": "Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. In this work, we introduce a unified framework, World-Model-Driven Diffusion Policy with Online Adaptive Learning (AdaWorldPolicy) to enhance robotic manipulation under dynamic conditions with minimal human involvement. Our core insight is that world models provide strong supervision signals, enabling online adaptive learning in dynamic environments, which can be complemented by force-torque feedback to mitigate dynamic force shifts. Our AdaWorldPolicy integrates a world model, an action expert, and a force predictor-all implemented as interconnected Flow Matching Diffusion Transformers (DiT). They are interconnected via the multi-modal self-attention layers, enabling deep feature exchange for joint learning while preserving their distinct modularity characteristics. We further propose a novel Online Adaptive Learning (AdaOL) strategy that dynamically switches between an Action Generation mode and a Future Imagination mode to drive reactive updates across all three modules. This creates a powerful closed-loop mechanism that adapts to both visual and physical domain shifts with minimal overhead. Across a suite of simulated and real-robot benchmarks, our AdaWorldPolicy achieves state-of-the-art performance, with dynamical adaptive capacity to out-of-distribution scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "published": "2026-02-23T17:12:25Z",
    "updated": "2026-02-23T17:12:25Z",
    "url": "https://arxiv.org/abs/2602.20057v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20057v1",
    "relevance_score": 0.0,
    "display_category": "LLMs & Agents",
    "summary": "Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20055v1",
    "title": "To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation",
    "authors": [
      "Apoorva Vashisth",
      "Manav Kulshrestha",
      "Pranav Bakshi",
      "Damon Conover",
      "Guillaume Sartoretti",
      "Aniket Bera"
    ],
    "abstract": "Visual navigation typically assumes the existence of at least one obstacle-free path between start and goal, which must be discovered/planned by the robot. However, in real-world scenarios, such as home environments and warehouses, clutter can block all routes. Targeted at such cases, we introduce the Lifelong Interactive Navigation problem, where a mobile robot with manipulation abilities can move clutter to forge its own path to complete sequential object- placement tasks - each involving placing an given object (eg. Alarm clock, Pillow) onto a target object (eg. Dining table, Desk, Bed). To address this lifelong setting - where effects of environment changes accumulate and have long-term effects - we propose an LLM-driven, constraint-based planning framework with active perception. Our framework allows the LLM to reason over a structured scene graph of discovered objects and obstacles, deciding which object to move, where to place it, and where to look next to discover task-relevant information. This coupling of reasoning and active perception allows the agent to explore the regions expected to contribute to task completion rather than exhaustively mapping the environment. A standard motion planner then executes the corresponding navigate-pick-place, or detour sequence, ensuring reliable low-level control. Evaluated in physics-enabled ProcTHOR-10k simulator, our approach outperforms non-learning and learning-based baselines. We further demonstrate our approach qualitatively on real-world hardware.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "published": "2026-02-23T17:10:00Z",
    "updated": "2026-02-23T17:10:00Z",
    "url": "https://arxiv.org/abs/2602.20055v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20055v1",
    "relevance_score": 0.0,
    "display_category": "LLMs & Agents",
    "summary": "Visual navigation typically assumes the existence of at least one obstacle-free path between start and goal, which must be discovered/planned by the robot.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20053v1",
    "title": "Decoupling Defense Strategies for Robust Image Watermarking",
    "authors": [
      "Jiahui Chen",
      "Zehang Deng",
      "Zeyu Zhang",
      "Chaoyang Li",
      "Lianchen Jia",
      "Lifeng Sun"
    ],
    "abstract": "Deep learning-based image watermarking, while robust against conventional distortions, remains vulnerable to advanced adversarial and regeneration attacks. Conventional countermeasures, which jointly optimize the encoder and decoder via a noise layer, face 2 inevitable challenges: (1) decrease of clean accuracy due to decoder adversarial training and (2) limited robustness due to simultaneous training of all three advanced attacks. To overcome these issues, we propose AdvMark, a novel two-stage fine-tuning framework that decouples the defense strategies. In stage 1, we address adversarial vulnerability via a tailored adversarial training paradigm that primarily fine-tunes the encoder while only conditionally updating the decoder. This approach learns to move the image into a non-attackable region, rather than modifying the decision boundary, thus preserving clean accuracy. In stage 2, we tackle distortion and regeneration attacks via direct image optimization. To preserve the adversarial robustness gained in stage 1, we formulate a principled, constrained image loss with theoretical guarantees, which balances the deviation from cover and previous encoded images. We also propose a quality-aware early-stop to further guarantee the lower bound of visual quality. Extensive experiments demonstrate AdvMark outperforms with the highest image quality and comprehensive robustness, i.e. up to 29\\%, 33\\% and 46\\% accuracy improvement for distortion, regeneration and adversarial attacks, respectively.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "published": "2026-02-23T17:02:55Z",
    "updated": "2026-02-23T17:02:55Z",
    "url": "https://arxiv.org/abs/2602.20053v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20053v1",
    "relevance_score": 0.0,
    "display_category": "Computer Vision",
    "summary": "Deep learning-based image watermarking, while robust against conventional distortions, remains vulnerable to advanced adversarial and regeneration attacks.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20052v1",
    "title": "Entropy in Large Language Models",
    "authors": [
      "Marco Scharringhausen"
    ],
    "abstract": "In this study, the output of large language models (LLM) is considered an information source generating an unlimited sequence of symbols drawn from a finite alphabet. Given the probabilistic nature of modern LLMs, we assume a probabilistic model for these LLMs, following a constant random distribution and the source itself thus being stationary. We compare this source entropy (per word) to that of natural language (written or spoken) as represented by the Open American National Corpus (OANC). Our results indicate that the word entropy of such LLMs is lower than the word entropy of natural speech both in written or spoken form. The long-term goal of such studies is to formalize the intuitions of information and uncertainty in large language training to assess the impact of training an LLM from LLM generated training data. This refers to texts from the world wide web in particular.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "published": "2026-02-23T17:02:45Z",
    "updated": "2026-02-23T17:02:45Z",
    "url": "https://arxiv.org/abs/2602.20052v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20052v1",
    "relevance_score": 0.0,
    "display_category": "LLMs & Agents",
    "summary": "In this study, the output of large language models (LLM) is considered an information source generating an unlimited sequence of symbols drawn from a finite alphabet.",
    "keyword_matches": []
  },
  {
    "arxiv_id": "2602.20051v1",
    "title": "SEAL-pose: Enhancing 3D Human Pose Estimation via a Learned Loss for Structural Consistency",
    "authors": [
      "Yeonsung Kim",
      "Junggeun Do",
      "Seunguk Do",
      "Sangmin Kim",
      "Jaesik Park",
      "Jay-Yoon Lee"
    ],
    "abstract": "3D human pose estimation (HPE) is characterized by intricate local and global dependencies among joints. Conventional supervised losses are limited in capturing these correlations because they treat each joint independently. Previous studies have attempted to promote structural consistency through manually designed priors or rule-based constraints; however, these approaches typically require manual specification and are often non-differentiable, limiting their use as end-to-end training objectives. We propose SEAL-pose, a data-driven framework in which a learnable loss-net trains a pose-net by evaluating structural plausibility. Rather than relying on hand-crafted priors, our joint-graph-based design enables the loss-net to learn complex structural dependencies directly from data. Extensive experiments on three 3D HPE benchmarks with eight backbones show that SEAL-pose reduces per-joint errors and improves pose plausibility compared with the corresponding backbones across all settings. Beyond improving each backbone, SEAL-pose also outperforms models with explicit structural constraints, despite not enforcing any such constraints. Finally, we analyze the relationship between the loss-net and structural consistency, and evaluate SEAL-pose in cross-dataset and in-the-wild settings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "published": "2026-02-23T17:00:35Z",
    "updated": "2026-02-23T17:00:35Z",
    "url": "https://arxiv.org/abs/2602.20051v1",
    "pdf_url": "https://arxiv.org/pdf/2602.20051v1",
    "relevance_score": 0.0,
    "display_category": "Computer Vision",
    "summary": "3D human pose estimation (HPE) is characterized by intricate local and global dependencies among joints.",
    "keyword_matches": []
  }
]